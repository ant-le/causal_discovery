# @package _global_
# BCNP "Biggest and Most Broad" run (Syntren configuration from Appendix C/Section 4.3).
# Replicates the "BCNP All Data" setup used for Syntren experiments.

name: "bcnp_paper_reproduction"

logger:
  wandb:
    enabled: true
    mode: "online"
    project: "causal_meta"
    entity: "lechuga-anton"
    name: "${name}"
    tags: ["bcnp", "paper", "reproduction", "all_data", "syntren"]

data:
  # -------------------------------------------------------------------------
  # TRAINING FAMILY: Mixture of all graph densities and function types.
  # "We generate graphs from the ER distribution as well as the scale-free distribution."
  # "Densities... sampled from 20 to 60."
  # "Edge function is sampled from NeuralNet or GPCDE... (and Linear implicit in 4.2)"
  # -------------------------------------------------------------------------
  train_family:
    name: bcnp_all_data_train
    n_nodes: 20

    # Graph: Mixture of ER (20, 40, 60 edges) and SF.
    # Weights assumed equal as per "equal probability".
    graph_cfg:
      type: mixture
      weights: [0.25, 0.25, 0.25, 0.25]
      generators:
        - type: er
          sparsity: 0.0526 # ~20 edges (20/380)
        - type: er
          sparsity: 0.1053 # ~40 edges (40/380)
        - type: er
          sparsity: 0.1579 # ~60 edges (60/380)
        - type: sf
          m: 2 # Scale-free

    # Mechanisms: Mixture of Linear, MLP (NeuralNet), GP (GPCDE).
    # "generate data from a mixture of all possible choices... with equal probability"
    mech_cfg:
      type: mixture
      weights: [0.33, 0.33, 0.34]
      factories:
        - type: linear
          weight_scale: 10.0 # From Appendix B.2
          noise_concentration: 2.0
          noise_rate: 2.0
        - type: mlp
          hidden_dim: 32 # From Appendix B.2 ("width of 32")
        - type: gp
          rff_dim: 256
          length_scale_range: [0.5, 2.0] # From B.2
          variance: 1.0

  # -------------------------------------------------------------------------
  # VALIDATION (In-Distribution)
  # -------------------------------------------------------------------------
  val_families:
    id_val:
      name: id_val
      n_nodes: 20
      graph_cfg:
        type: er
        sparsity: 0.1053 # Mid-range (40 edges)
      mech_cfg:
        type: mlp
        hidden_dim: 32

  # -------------------------------------------------------------------------
  # TEST FAMILIES (From BCNP paper audit)
  # -------------------------------------------------------------------------
  test_families:
    # 1. Functional Shifts
    ood_square:
      name: ood_square
      n_nodes: 20
      graph_cfg:
        type: er
        sparsity: 0.1
      mech_cfg:
        type: square
        weight_scale: 1.0
        noise_scale: 0.1

    # 2. Chaos
    ood_logistic_map:
      name: ood_logistic_map
      n_nodes: 20
      graph_cfg:
        type: er
        sparsity: 0.1
      mech_cfg:
        type: logistic_map
        weight_scale: 1.0

    # 3. PNL
    ood_pnl_tanh:
      name: ood_pnl_tanh
      n_nodes: 20
      graph_cfg:
        type: er
        sparsity: 0.1
      mech_cfg:
        type: pnl
        nonlinearity_type: tanh
        inner_config:
          type: linear

    # 4. Structural
    ood_sbm_strong:
      name: ood_sbm_strong
      n_nodes: 20
      graph_cfg:
        type: sbm
        n_blocks: 4
        p_intra: 0.6
        p_inter: 0.01
      mech_cfg:
        type: linear

  # Training details
  # "All datasets contain 1000 samples" (Appendix B.2)
  # "We use 500,000 datasets to train in total" (Appendix C)
  # With batch_size=1 (repo constraint), max_steps = 500,000 matches total datasets.
  samples_per_task: 1000

  seeds_val: [1000, 1001, 1002, 1003, 1004]
  seeds_test: [2000, 2001, 2002, 2003, 2004]
  base_seed: 42
  safety_checks: true
  num_workers: 4
  pin_memory: true
  normalize_data: true # "we normalise all variables after generation" (B.2)
  batch_size_train: 1
  batch_size_val: 1
  batch_size_test: 1
  batch_size_test_interventional: 1

model:
  type: "bcnp"
  num_nodes: 20

  # "increase the number of attention heads to 16 and increase the feedforward width... to 2048" (Appendix C)
  # "width of 512 for the attention layers" (Section 4.2, assumed kept for Syntren unless specified otherwise)
  d_model: 512
  dim_feedforward: 2048
  nhead: 16

  # "All models used 4 layers in their encoder" (2 blocks of Sample+Node)
  # "BCNP also have 4 decoder layers"
  num_layers: 4
  num_layers_decoder: 4

  dropout: 0.1
  emb_depth: 2
  use_positional_encoding: false

  # "For BCNP, we used 100 permutation samples... and maximum of 1000 sinkhorn iterations" (B.1)
  n_perm_samples: 100
  sinkhorn_iter: 1000
  q_before_l: true

trainer:
  lr: 0.0001
  weight_decay: 1e-4
  accumulate_grad_batches: 1

  # 500,000 datasets / batch_size 1 = 500,000 steps
  max_steps: 500000

  log_every_n_steps: 100
  val_check_interval: 5000 # Check every 5k steps
  checkpoint_every_n_steps: 50000

  amp: true # "train in bfloat 16" (B.1)
  amp_dtype: bf16
  grad_clip_norm: 1.0

  scheduler: cosine
  scheduler_t_max: 500000
  scheduler_eta_min: 0.0

inference:
  n_samples: 20

hydra:
  run:
    dir: ${oc.env:CAUSAL_META_RUN_DIR,experiments/runs/${name}/${now:%Y-%m-%d}/${now:%H-%M-%S}}
  sweep:
    dir: ${oc.env:CAUSAL_META_SWEEP_DIR,experiments/runs/${name}/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}}
    subdir: ${hydra.job.num}

# @package trainer

lr: 0.0001
weight_decay: 1e-4
accumulate_grad_batches: 1
max_steps: 30
log_every_n_steps: 1
val_check_interval: 10
checkpoint_every_n_steps: 0
amp: true
amp_dtype: bf16
grad_clip_norm: 1.0
scheduler: cosine
scheduler_t_max: 30
scheduler_eta_min: 0.0
scheduler_warmup_ratio: 0.1
scheduler_warmup_start_factor: 0.001
regulariser_update_interval: 500

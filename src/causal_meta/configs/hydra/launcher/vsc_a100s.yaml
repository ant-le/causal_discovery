# @package hydra.launcher
_target_: hydra_plugins.hydra_submitit_launcher.submitit_launcher.SlurmLauncher
submitit_folder: ${hydra.sweep.dir}/.submitit

# Job Resources (Per Model/Task)
timeout_min: 1440 # 24 hours
max_num_timeout: 3 # Auto-requeue timed-out jobs (up to 4 total attempts)
cpus_per_task: 5 # CPUs per GPU task
tasks_per_node: 4 # 4 tasks per node (matches GPUs)
gpus_per_task: 1
mem_gb: 200 # 50G * 4 = 200G total RAM per node
nodes: 1

# Scheduler Directives
partition: "GPU-a100"
gres: "gpu:a100:4"
name: "cm_${model.id}"
# account: <YOUR_ACCOUNT>   # Uncomment if required

# specific SBATCH flags
additional_parameters:
  requeue: true
  mail-user: ${oc.env:SLURM_MAIL_USER,e12229950@student.tuwien.ac.at}
  mail-type: "BEGIN,END,FAIL"

setup:
  - export OMP_NUM_THREADS=${hydra.launcher.cpus_per_task}
  - export HYDRA_FULL_ERROR=1
  # Map SLURM env vars to PyTorch distributed (required for DDP init via env://)
  - export RANK=$SLURM_PROCID
  - export LOCAL_RANK=$SLURM_LOCALID
  - export WORLD_SIZE=$SLURM_NTASKS
  - export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
  - export MASTER_PORT=${oc.env:MASTER_PORT,29500}

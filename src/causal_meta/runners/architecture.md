# Runner Architecture & Design

## 1. Overview
The runner module bridges the `datasets` (Infinite Streams/Fixed Sets) and `models` (Amortized/Instance-specific) to execute experiments. It owns the outer-loop logic: training control flow, evaluation protocols, metric computation, and artifact storage.

## 2. Component Design

### 2.1 Configuration (`ExperimentConfig`)
To ensure reproducibility, we require a top-level configuration object (hydra-compatible) that composes:
-   `name`: `str` (Unique identifier for the run, used for directory naming).
-   `model`: `ModelConfig` (passed to `ModelFactory`).
-   `data`: `DataModuleConfig` (passed to `CausalMetaModule`).
-   `trainer`: `TrainerConfig` (optimization params, step budget, logging, checkpointing).
-   `inference`: `InferenceConfig` (evaluation-time parameters, e.g. `n_samples`, MCMC steps).

### 2.2 Training Objective (Loss) Strategy
To allow model-specific regularization (e.g. acyclicity constraints in Avici) and objectives, `BaseModel` handles the loss calculation via `calculate_loss`.
-   **Mechanism:** The Runner calls `model.calculate_loss(output, target, **kwargs)`.
-   **Responsibility:** The model defines its own supervised objective and any auxiliary losses.
-   **Polymorphism:** `AviciModel` can implement complex acyclicity regularization while `BCNP` implements standard NLL, without the Runner needing to know the details.

### 2.3 Inference & Evaluation Flow
To handle computationally expensive baselines (MCMC/VI) efficiently, we decouple **Inference** (posterior approximation) from **Evaluation** (metric computation).

-   **Protocol:**
    1.  **Amortized Models:** `evaluation.py` calls `model.sample(x)` directly (fast).
    2.  **Explicit Bayesian Models:** 
        -   Step 1: Run `inference.py` to generate and save `InferenceResult` artifacts (posterior samples/params).
        -   Step 2: `evaluation.py` consumes these artifacts to compute metrics. 
        -   *Fallback:* `evaluation.py` can trigger `model.sample()` if artifacts are missing, but this is discouraged for heavy baselines.
-   **State:** The `sample()` method must remain side-effect free regarding global model state.

### 2.4 Scripts & Roles

#### A. `pre_training.py` (The Meta-Learner Loop)
**Target:** Models where `needs_pretraining=True` (e.g., Avici, BCNP).
**Flow:**
1.  **Init:** Load `ExperimentConfig`.
2.  **Data:** Call `CausalMetaModule.train_dataloader()`.
    *   *Constraint:* This is an **infinite stream**. The loop is controlled by `trainer.max_steps`.
3.  **Model:** Instantiate via `ModelFactory`.
4.  **Objective:** Instantiate via `ObjectiveFactory` (e.g., `NLL`, `BCE`).
5.  **Training Loop:** 
    -   Fetch batch `(x, adj)` from iterator.
    -   Forward: `out = model(x)`.
    -   Compute: `loss, logs = objective(out, adj)`.
    -   Backprop & Optimizer Step.
    -   *Logging:* Log metrics every `log_every_n_steps`.
6.  **Validation:** 
    -   Run periodic evaluation on `CausalMetaModule.test_dataloader()` every `val_check_interval` (steps).
7.  **Artifacts:** 
    -   Save `last.pt`, `best.pt`, and `config.yaml`.

#### B. `evaluation.py` (The Benchmarker)
**Target:** ALL models (Amortized & Instance-Specific).
**Flow:**
1.  **Init:** Load `ExperimentConfig`.
    -   If Amortized: Load weights from `checkpoint_path`.
2.  **Data:** Iterate through families in `CausalMetaModule.test_dataloader()`.
3.  **Inference:**
    -   **Amortized:** Call `model.sample(x, num_samples)`.
    -   **Explicit Bayesian:** Load `InferenceResult` from disk (generated by `inference.py`). Only run `model.sample()` if artifacts are missing.
4.  **Metrics:** 
    -   **Graph:** SHD, SID, F1, AUROC, Precision, Recall.
    -   **Likelihood:** NLL (if supported).
5.  **Output:** 
    -   Save `metrics.json`.

#### C. `inference.py` (Bayesian Inference Engine)
**Target:** Explicit Bayesian Models (MCMC/VI).
-   **Role:** Dedicated script for executing compute-intensive posterior approximation (MCMC, Variational Inference) for non-amortized models. It handles the optimization/sampling loop for individual instances where amortized forward passes are not applicable.
-   **Artifacts:** Saves the resulting posterior samples or distribution parameters to `inference/` for downstream evaluation.
-   **Constraint:** This script is strictly for inference logic; it excludes debugging utilities or visualization code.

#### D. Visualization
**Target:** Debugging, Post-hoc Analysis, and Figure Generation.
-   **Tooling:** Use **Jupyter Notebooks** that load artifacts (`metrics.json`, `inference/` results) to generate plots and inspect specific SCM instances. No dedicated runner script is provided.

### 2.5 Protocols (`src/causal_meta/runners/`)
*   `eval_metrics.py`:
    *   `compute_graph_metrics(pred_adj: Tensor, true_adj: Tensor) -> Dict`
*   `train_metrics.py`:
    *   `compute_predictive_nll(model, x_test) -> float`

## 3. Technical Constraints
1.  **Infinite Data Handling (PyTorch Lightning/Raw):** 
    -   **Control:** Use `max_steps` and `val_check_interval` (int). Avoid epoch-based logic.
    -   **Resuming:** Checkpoints do not restore `IterableDataset` iterator state. Resuming restarts the stream from `base_seed`.
    -   **Validation:** Set `limit_val_batches` to avoid infinite validation loops if the test loader were also iterable (though `MetaFixedDataset` is not).
2.  **Parallelization:**
    -   `DataModule` handles rank-aware seeding.
    -   Use `DistributedDataParallel` (DDP) correctly; ensure rank 0 handles logging/checkpointing.
3.  **Artifacts:**
    -   Directory: `experiments/<name>/`
    -   Structure: 
        -   `checkpoints/` (Model weights)
        -   `results/` (Metrics)
        -   `inference/` (Posterior samples/parameters for MCMC/VI)
        -   `config.yaml`
